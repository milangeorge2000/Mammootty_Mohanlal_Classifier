{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "tinkerhub'.ipynb",
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "acd888601bfa4480b6dc3b9c22e71f03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_877ad9a6672846938d0cfd162f317c4b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_298a5a6a973f44a6ba950d2a3f829127",
              "IPY_MODEL_6fee3f7f1f3c4975a8224a8f98aeb8d3"
            ]
          }
        },
        "877ad9a6672846938d0cfd162f317c4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "298a5a6a973f44a6ba950d2a3f829127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0ea1b3cef8954572b0dfe27d56acecfc",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 111898327,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 111898327,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9a62c99d5df944c48ecd72dcd94609e5"
          }
        },
        "6fee3f7f1f3c4975a8224a8f98aeb8d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d4828aa94f4840ad9414fc05d9bd2299",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 107M/107M [00:01&lt;00:00, 57.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_69f203172ccb470881fd7e33bbb2d5aa"
          }
        },
        "0ea1b3cef8954572b0dfe27d56acecfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9a62c99d5df944c48ecd72dcd94609e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d4828aa94f4840ad9414fc05d9bd2299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "69f203172ccb470881fd7e33bbb2d5aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II0-BLcAQDTJ",
        "outputId": "8e60154b-3b76-4cfe-9af3-f599a99e6717"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        " \n",
        "drive.mount('/content/gdrive')\n",
        "%cd '/content/gdrive/My Drive/tinkerhub'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/tinkerhub\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIpQfNVoSQtW",
        "outputId": "38c7403d-861d-44a0-958f-a736d7b048ce"
      },
      "source": [
        "!pip install facenet_pytorch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: facenet_pytorch in /usr/local/lib/python3.7/dist-packages (2.5.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (0.9.1+cu101)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (2.23.0)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet_pytorch) (1.8.1+cu101)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchvision->facenet_pytorch) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dps6WvsSNwx"
      },
      "source": [
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTFOtVQnQDTK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "acd888601bfa4480b6dc3b9c22e71f03",
            "877ad9a6672846938d0cfd162f317c4b",
            "298a5a6a973f44a6ba950d2a3f829127",
            "6fee3f7f1f3c4975a8224a8f98aeb8d3",
            "0ea1b3cef8954572b0dfe27d56acecfc",
            "9a62c99d5df944c48ecd72dcd94609e5",
            "d4828aa94f4840ad9414fc05d9bd2299",
            "69f203172ccb470881fd7e33bbb2d5aa"
          ]
        },
        "outputId": "e686e0cc-2792-4b9a-9b7e-1b7091f8b737"
      },
      "source": [
        "\n",
        "mtcnn = MTCNN(image_size=240, margin=0, min_face_size=20) # initializing mtcnn for face detection\n",
        "resnet = InceptionResnetV1(pretrained='vggface2').eval() # initializing resnet for face img to embeding conversion\n",
        "\n",
        "dataset=datasets.ImageFolder('train') # photos folder path \n",
        "idx_to_class = {i:c for c,i in dataset.class_to_idx.items()} # accessing names of peoples from folder names\n",
        "\n",
        "def collate_fn(x):\n",
        "    return x[0]\n",
        "\n",
        "loader = DataLoader(dataset, collate_fn=collate_fn)\n",
        "\n",
        "face_list = [] # list of cropped faces from photos folder\n",
        "name_list = [] # list of names corrospoing to cropped photos\n",
        "embedding_list = [] # list of embeding matrix after conversion from cropped faces to embedding matrix using resnet\n",
        "\n",
        "for img, idx in loader:\n",
        "    face, prob = mtcnn(img, return_prob=True) \n",
        "    if face is not None and prob>0.90: # if face detected and porbability > 90%\n",
        "        emb = resnet(face.unsqueeze(0)) # passing cropped face into resnet model to get embedding matrix\n",
        "        embedding_list.append(emb.detach()) # resulten embedding matrix is stored in a list\n",
        "        name_list.append(idx_to_class[idx]) # names are stored in a list\n",
        "        \n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "acd888601bfa4480b6dc3b9c22e71f03",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=111898327.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbMnwffnQDTL"
      },
      "source": [
        "data = [embedding_list, name_list]\n",
        "torch.save(data, 'data2.pt') # saving data.pt file"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBuvRHu0OtFr"
      },
      "source": [
        "mtcnn = MTCNN(image_size=240, margin=0, min_face_size=20) # initializing mtcnn for face detection\n",
        "resnet = InceptionResnetV1(pretrained='vggface2').eval() # initializing resnet for face img to embeding conversion"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wsnaz4MJXiR"
      },
      "source": [
        "torch.save(mtcnn,\"data1.pt\")\n",
        "torch.save(resnet,\"data3.pt\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWAg826-JX1r"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsXeEB88QDTM"
      },
      "source": [
        "### Matching face id of the given photo with available data from data2.pt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO-5EHuxQDTM",
        "outputId": "914c45ba-29e8-4306-be39-f54a3422d5de"
      },
      "source": [
        "\n",
        "def face_match(img_path, data_path): # img_path= location of photo, data_path= location of data.pt \n",
        "    # getting embedding matrix of the given img\n",
        "    img = Image.open(img_path)\n",
        "    face, prob = mtcnn(img, return_prob=True) # returns cropped face and probability\n",
        "    emb = resnet(face.unsqueeze(0)).detach() # detech is to make required gradient false\n",
        "    saved_data = torch.load('data2.pt') # loading data.pt file\n",
        "    embedding_list = saved_data[0] # getting embedding data\n",
        "    name_list = saved_data[1] # getting list of names\n",
        "    dist_list = [] # list of matched distances, minimum distance is used to identify the person\n",
        "    \n",
        "    for idx, emb_db in enumerate(embedding_list):\n",
        "        dist = torch.dist(emb, emb_db).item()\n",
        "        dist_list.append(dist)\n",
        "        \n",
        "    idx_min = dist_list.index(min(dist_list))\n",
        "    return (name_list[idx_min], min(dist_list))\n",
        "\n",
        "\n",
        "result = face_match('download (16).jpg', 'data2.pt')\n",
        "\n",
        "print('Face matched with: ',result[0], 'With distance: ',result[1])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Face matched with:  unknown With distance:  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ6kxJATl3J2"
      },
      "source": [
        "y_test,y_pred = [],[]\n",
        "fmammooka , fmohanlal , funknown= [] , [] ,[]\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxaDJadQK9Mt"
      },
      "source": [
        "from os import walk"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we1H-U3BKinx"
      },
      "source": [
        "for (dirpath, dirnames, filenames) in walk('test/mammootty'):\n",
        "    fmammooka.extend(filenames)\n",
        "    break"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJuV7xpMKiww"
      },
      "source": [
        "for (dirpath, dirnames, filenames) in walk('test/mohanlal'):\n",
        "    fmohanlal.extend(filenames)\n",
        "    break"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBVhJ_V0Muzy"
      },
      "source": [
        "for (dirpath, dirnames, filenames) in walk('test/unknown'):\n",
        "    funknown.extend(filenames)\n",
        "    break"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmpucW6TLjy8"
      },
      "source": [
        "for i in fmammooka:\n",
        "  y_test.append(0)\n",
        "  result = face_match(f\"test/mammootty/{i}\", 'data2.pt')\n",
        "  if(result[0] == \"mammootty\"):\n",
        "    y_pred.append(0)\n",
        "  elif(result[0] == \"mohanlal\"):\n",
        "    y_pred.append(1)\n",
        "  elif(result[0] == \"unknown\"):\n",
        "    y_pred.append(2)\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg0UQXPLNtvZ"
      },
      "source": [
        "for i in fmohanlal:\n",
        "  y_test.append(1)\n",
        "  result = face_match(f\"test/mohanlal/{i}\", 'data2.pt')\n",
        "  if(result[0] == \"mammootty\"):\n",
        "    y_pred.append(0)\n",
        "  elif(result[0] == \"mohanlal\"):\n",
        "    y_pred.append(1)\n",
        "  elif(result[0] == \"unknown\"):\n",
        "    y_pred.append(2)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYmneE0NQinQ"
      },
      "source": [
        "for i in funknown:\n",
        "  y_test.append(2)\n",
        "  result = face_match(f\"test/unknown/{i}\", 'data2.pt')\n",
        "  if(result[0] == \"mammootty\"):\n",
        "    y_pred.append(0)\n",
        "  elif(result[0] == \"mohanlal\"):\n",
        "    y_pred.append(1)\n",
        "  elif(result[0] == \"unknown\"):\n",
        "    y_pred.append(2)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCMVkPLHS-Kl",
        "outputId": "f9844719-1777-4464-b2d6-145238a01701"
      },
      "source": [
        "print(accuracy_score(y_test,y_pred))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jYLxSH9Qk3I",
        "outputId": "10e12285-e749-47f2-c5a9-0c7a55f740be"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88        19\n",
            "           1       0.88      1.00      0.93        21\n",
            "           2       1.00      0.40      0.57        10\n",
            "\n",
            "    accuracy                           0.86        50\n",
            "   macro avg       0.90      0.78      0.79        50\n",
            "weighted avg       0.88      0.86      0.84        50\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BjCxUNWS5HH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}